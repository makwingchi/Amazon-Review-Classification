{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Review Analysis and Classification\n",
    "\n",
    "The training set for this analysis is the text from 100,000 reviews from Amazon.com, their timestamps, and their star ratings. The high level goal of this analysis is to use the textual and temporal data to predict the star ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel, KeyedVectors\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from datetime import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read stuff\n",
    "reviews_dict = corpora.Dictionary.load(\"reviews.dict\")\n",
    "reviews_bow = corpora.MmCorpus('train_reviews.mm')\n",
    "reviews_times  = np.load('train_times.npy')\n",
    "reviews_times.shape = (len(reviews_bow),1)\n",
    "reviews_wc = corpora.MmCorpus('reviews_wc.mm')\n",
    "reviews_sppmi_300 = np.load('reviews_sppmi_300.npy')\n",
    "reviews_wv = KeyedVectors.load(\"word_vectors.wv\", mmap='r')\n",
    "y = np.vstack((np.repeat(1, 4000), np.repeat(2, 4000), np.repeat(3, 4000), np.repeat(4, 4000), np.repeat(5, 4000)))\n",
    "y = np.repeat(y, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Word vectors\n",
    "\n",
    "Performing several optimizations on a word-context shifted positive pointwise mutual information ($SPPMI$) matrix, including decomposing with SVD, is mathematically equivalent to a neural network that learns word embeddings using skip gram with negative sampling ($SGNS$).  We are going to implement each of these here and compare against a pure bag of words ($BOW$) model as a baseline.\n",
    "\n",
    "To begin, let's make a corpus out of a toy dataset: the 5 computer science and 4 math article titles. After lower casing, tokenizing, and stop wording, the corpus looks like `titles` in the cell below. Then, we create a dictionary and a sparse document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
       " [(1, 1), (5, 2), (8, 1)],\n",
       " [(3, 1), (6, 1), (7, 1)],\n",
       " [(9, 1)],\n",
       " [(9, 1), (10, 1)],\n",
       " [(9, 1), (10, 1), (11, 1)],\n",
       " [(4, 1), (10, 1), (11, 1)]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titles = [['human', 'interface', 'computer'],\n",
    "          ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
    "          ['eps', 'user', 'interface', 'system'],\n",
    "          ['system', 'human', 'system', 'eps'],\n",
    "          ['user', 'response', 'time'],\n",
    "          ['trees'],\n",
    "          ['graph', 'trees'],\n",
    "          ['graph', 'minors', 'trees'],\n",
    "          ['graph', 'minors', 'survey']]\n",
    "\n",
    "titles_dict = corpora.Dictionary(titles)\n",
    "titles_bow = [titles_dict.doc2bow(title) for title in titles]\n",
    "display(titles_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Sparse to dense\n",
    "\n",
    "To get the term-document matrix, we need to convert this matrix to its dense form. We will write a function `densify` that takes as input:\n",
    "\n",
    "1. a sparse matrix in the format of `titles_bow` above\n",
    "3. an integer number of columns\n",
    "\n",
    "and returns a NumPy array. Please note that `titles_bow` is a document-term matrix, not a term-document matrix, so we transpose it in the second cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def densify(sparse, columns):\n",
    "    orig = np.zeros((len(sparse), columns))\n",
    "    count = 0\n",
    "    for i in sparse:\n",
    "        for j in i:\n",
    "            orig[count][j[0]] = j[1]\n",
    "        count += 1\n",
    "    \n",
    "    return orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 1. 2. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "td = densify(titles_bow, len(titles_dict)).transpose()\n",
    "print(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 9)\n"
     ]
    }
   ],
   "source": [
    "print(td.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Counting words\n",
    "\n",
    "We will write a function `count_words` that takes as input:\n",
    "\n",
    "1. a dictionary in the format of `titles_dict` above\n",
    "2. a bag of words corpus in the format of `titles_bow` above\n",
    "\n",
    "and returns a list called `counts` that has the total occurrences of each word in the corpus, in the order of the original word indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(gsdict, gsbow):\n",
    "    orig = np.zeros((len(gsbow), len(gsdict)))\n",
    "    count = 0\n",
    "    for i in gsbow:\n",
    "        for j in i:\n",
    "            orig[count][j[0]] = j[1]\n",
    "        count += 1\n",
    "    \n",
    "    return np.sum(orig, axis=0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titles_counts = count_words(titles_dict, titles_bow)\n",
    "display(len(titles_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0, 2.0, 2.0, 2.0, 2.0, 4.0, 2.0, 3.0, 2.0, 3.0, 3.0, 2.0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(titles_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below makes the counts for the 100,000 Amazon reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_counts = count_words(reviews_dict, reviews_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.3: The word-context matrix\n",
    "\n",
    "Now, we will write a function `word_context` that takes as input:\n",
    "\n",
    "1. a dictionary in the format of `titles_dict` above\n",
    "2. a corpus in the format of `titles` above\n",
    "3. a window size (integer)\n",
    "\n",
    "and creates a **sparse** word-context matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_context(gsdict, gscorpus, window):\n",
    "    orig = np.zeros((len(gsdict), len(gsdict)))\n",
    "    \n",
    "    for doc in gscorpus:\n",
    "        for i in range(len(doc)):\n",
    "            for j in range(i-window, i+window+1):\n",
    "                if j >=0 and j <= len(doc)-1 and j != i:\n",
    "                    #print(j)\n",
    "                    context = gsdict.doc2idx([doc[i]])[0]\n",
    "                    #print(context)\n",
    "                    word = gsdict.doc2idx([doc[j]])[0]\n",
    "                    #print(word)\n",
    "                    orig[word][context] += 1\n",
    "    \n",
    "    lst1 = []\n",
    "    for i in orig:\n",
    "        lst2 = []\n",
    "        for word, count in enumerate(i):\n",
    "            if count > 0:\n",
    "                lst2.append((word, count))\n",
    "        \n",
    "        lst1.append(lst2)\n",
    "    \n",
    "    return lst1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1, 1.0), (2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0), (7, 1.0)],\n",
       " [(0, 1.0), (2, 1.0), (5, 2.0), (8, 1.0)],\n",
       " [(0, 1.0), (1, 1.0), (5, 1.0), (7, 1.0), (8, 1.0)],\n",
       " [(0, 1.0), (5, 1.0), (6, 2.0), (7, 1.0)],\n",
       " [(0, 1.0), (7, 1.0), (10, 1.0), (11, 1.0)],\n",
       " [(0, 1.0),\n",
       "  (1, 2.0),\n",
       "  (2, 1.0),\n",
       "  (3, 1.0),\n",
       "  (5, 2.0),\n",
       "  (6, 1.0),\n",
       "  (7, 2.0),\n",
       "  (8, 1.0)],\n",
       " [(3, 2.0), (5, 1.0), (7, 1.0)],\n",
       " [(0, 1.0), (2, 1.0), (3, 1.0), (4, 1.0), (5, 2.0), (6, 1.0), (8, 1.0)],\n",
       " [(1, 1.0), (2, 1.0), (5, 1.0), (7, 1.0)],\n",
       " [(10, 2.0), (11, 1.0)],\n",
       " [(4, 1.0), (9, 2.0), (11, 2.0)],\n",
       " [(4, 1.0), (9, 1.0), (10, 2.0)]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titles_wc = word_context(titles_dict, titles, 2)\n",
    "display(titles_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 1., 0., 0., 2., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 1., 2., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1.],\n",
       "       [1., 2., 1., 1., 0., 2., 1., 2., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 2., 0., 1., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 1., 1., 1., 2., 1., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 1.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 2., 0., 2.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 2., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(densify(titles_wc, len(titles_dict))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.4: Enhancing the word-context matrix\n",
    "\n",
    "There are a number of possible enhancements for word-context matrices. Here, we will write a function `sppmi` that takes as input:\n",
    "\n",
    "1. a sparse word-context matrix in the format of `titles_wc` above\n",
    "3. a counts dictionary\n",
    "4. a float `logk`\n",
    "\n",
    "and returns a new sparse word-context matrix with the values in the matrix replaced by shifted positive pointwise mutual informations ($SPPMI$). The formula is:\n",
    "\n",
    "$$SPPMI = \\max(\\log (\\frac{\\#(w,c) |D|}{\\#(w)\\#(c)}) - \\log(k), 0)$$\n",
    "\n",
    "where $\\#(w,c)$ is the count of word $w$ in context $c$ (the original count from the last section), $\\#(w)$ is the count of word $w$, $\\#(c)$ is the count of word $c$ (both come from `titles_counts`), $|D|$ is the length of the corpus, and $k$ is a free hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sppmi(gswc, counts, logk):\n",
    "    word = 0\n",
    "    d = len(counts)\n",
    "    lst1 = []\n",
    "    for row in gswc:\n",
    "        lst2 = []\n",
    "        for col in row:\n",
    "            context = col[0]\n",
    "            wc_count = col[1]\n",
    "            w_count, c_count = counts[word], counts[context]\n",
    "            left, right = np.log((wc_count*d)/(w_count*c_count)) - logk, 0\n",
    "            if left >= right:\n",
    "                result = left\n",
    "            else:\n",
    "                result = right\n",
    "            \n",
    "            lst2.append((context, result))\n",
    "        \n",
    "        word += 1\n",
    "        lst1.append(lst2)\n",
    "    \n",
    "    #print(lst1)\n",
    "    \n",
    "    return lst1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_sppmi = sppmi(titles_wc, titles_counts, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 1.1, 1.1, 1.1, 1.1, 0.4, 0. , 0.7, 0. , 0. , 0. , 0. ],\n",
       "       [1.1, 0. , 1.1, 0. , 0. , 1.1, 0. , 0. , 1.1, 0. , 0. , 0. ],\n",
       "       [1.1, 1.1, 0. , 0. , 0. , 0.4, 0. , 0.7, 1.1, 0. , 0. , 0. ],\n",
       "       [1.1, 0. , 0. , 0. , 0. , 0.4, 1.8, 0.7, 0. , 0. , 0. , 0. ],\n",
       "       [1.1, 0. , 0. , 0. , 0. , 0. , 0. , 0.7, 0. , 0. , 0.7, 1.1],\n",
       "       [0.4, 1.1, 0.4, 0.4, 0. , 0.4, 0.4, 0.7, 0.4, 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , 1.8, 0. , 0.4, 0. , 0.7, 0. , 0. , 0. , 0. ],\n",
       "       [0.7, 0. , 0.7, 0.7, 0.7, 0.7, 0.7, 0. , 0.7, 0. , 0. , 0. ],\n",
       "       [0. , 1.1, 1.1, 0. , 0. , 0.4, 0. , 0.7, 0. , 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. , 0.7],\n",
       "       [0. , 0. , 0. , 0. , 0.7, 0. , 0. , 0. , 0. , 1. , 0. , 1.4],\n",
       "       [0. , 0. , 0. , 0. , 1.1, 0. , 0. , 0. , 0. , 0.7, 1.4, 0. ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titles_dense_sppmi = densify(titles_sppmi, len(titles_dict))\n",
    "display(titles_dense_sppmi.round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below converts the counts to $SPPMI$s in the word-context matrix for the 100,000 Amazon reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sppmi = sppmi(reviews_wc, reviews_counts, np.log(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.5: Sparse SVD/PCA/LSA/LSI\n",
    "\n",
    "In the cell below, we write a function called `reconstruction` that takes as input:\n",
    "\n",
    "1. a sparse matrix\n",
    "2. a gensim dictionary\n",
    "2. a cutoff for PCA\n",
    "\n",
    "The function will compute the norm of the difference of the reconstructed matrix and the original, and return that dividing by the norm of the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction(sparse, gsdict, cutoff):\n",
    "    model = LsiModel(sparse, id2word=gsdict, num_topics=cutoff)\n",
    "    u = model.projection.u\n",
    "    s = np.diag(model.projection.s)\n",
    "    v = (densify(model[sparse], cutoff)/model.projection.s).T\n",
    "    \n",
    "    new_matrix = u@s@v\n",
    "    densified_sparse = densify(sparse, len(gsdict))\n",
    "    \n",
    "    diff = new_matrix.T - densified_sparse\n",
    "    \n",
    "    return np.linalg.norm(diff) / np.linalg.norm(densified_sparse)\n",
    "\n",
    "cutoff = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reconstruction error with 2 components on the the toy dataset is 0.6569296073921537\n"
     ]
    }
   ],
   "source": [
    "error = reconstruction(titles_bow, titles_dict, cutoff)\n",
    "print(\"The reconstruction error with\", cutoff, \"components on the the toy dataset is\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reconstruction error with 9 components on the the toy dataset is 1.1298655177653523e-15\n"
     ]
    }
   ],
   "source": [
    "cutoff = 9\n",
    "error = reconstruction(titles_bow, titles_dict, cutoff)\n",
    "print(\"The reconstruction error with\", cutoff, \"components on the the toy dataset is\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.6: Assembling the dense representation\n",
    "\n",
    "The last step in this process is to combine our two datasets, namely the document-term matrix and the word-context matrix. The appeal is that rather than reducing dimensionality by choosing a very small vocabulary, we can select relevant features from a vector space that contains all of the words. However, this requires a heavy assumption: a document representation is the sum of the representations of its words. This does not allow any non-compositionality of language and it makes each word equally important.\n",
    "\n",
    "In the cell below, we will write a function `vec2doc` that takes as input:\n",
    "\n",
    "1. a bag of words corpus in the format of `titles_bow` above\n",
    "2. a list of word vectors in proper order (by dictionary index) like `reviews_wv` above\n",
    "\n",
    "and returns a dense matrix. This matrix contains one vector per document which was computed by summing all of the vectors corresponding to the words in the document (including repeats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec2doc(gsbow, vectors):\n",
    "    y_size = vectors.shape[1]\n",
    "    length = len(gsbow)\n",
    "    orig = np.zeros((length, y_size))\n",
    "    \n",
    "    i = 0\n",
    "    for doc in gsbow:\n",
    "        for word in doc:\n",
    "            #print(word)\n",
    "            orig[i] = orig[i] + vectors[word[0]] * word[1]\n",
    "        i += 1\n",
    "            \n",
    "    return orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_sppmi = vec2doc(reviews_bow, reviews_sppmi_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(vecs_sppmi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature selection and hyperparameter tuning\n",
    "\n",
    "### Step 2.1: Assembling the other dense representation\n",
    "\n",
    "In the cell below, we will write a function `use_dict` that takes as input:\n",
    "\n",
    "1. a dictionary in the format of `titles_dict` above\n",
    "2. `KeyedVector word2vec` embeddings, as in `reviews_wv`\n",
    "\n",
    "and returns a list of vectors indexed using the input dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_dict(gsdict, vectors):\n",
    "    orig = np.zeros((len(gsdict), 300))\n",
    "    \n",
    "    for i in range(len(reviews_dict)):\n",
    "        word = gsdict[i]\n",
    "        if word in vectors:\n",
    "            orig[i] = vectors[word]\n",
    "        \n",
    "    return orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sgns_300 = use_dict(reviews_dict, reviews_wv)\n",
    "vecs_sgns = vec2doc(reviews_bow, reviews_sgns_300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: PCA on pure BOW\n",
    "\n",
    "While the $SPPMI$ and $SGNS$ models are already dimensionality reduced, it would be a good idea to also run PCA on the $BOW$ baseline. Here, we will train a gensim `LsiModel` on `reviews_bow` using `reviews_dict` as the dictionary and 1000 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cutoff = 1000\n",
    "\n",
    "model = LsiModel(reviews_bow, max_cutoff, reviews_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is to plot the explained variance versus number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl8VPW9//HXJ6xJWJKQBAIJJCwCgijKIi51a12w1Wq1dalSpdre2mpXW3/1trXt7XZte7tp662t1LordW2rXkStWlkFZBFE1kAgCWRjCWT5/P44JzSmSRgSJpOZeT8fj3lkzpkzcz4nB+ad7/ec8z3m7oiIiLSUEusCRESke1JAiIhIqxQQIiLSKgWEiIi0SgEhIiKtUkCIiEirFBAi3YiZbTKzD3byM/aY2cijVZMkLwWExL3wS3V/+MW408z+aGb9mr1+npm9amY1ZlZmZq+Y2UUtPuNMM3MzuzXCdRaZWaOZ3XW0t6ez3L2fu2+IdR0S/xQQkig+4u79gBOBqcDtAGZ2GfAY8CcgHxgMfAv4SIv3zwJ2hz8jcS1QAVxhZn06Xb1IN6SAkITi7tuAvwETzcyAnwHfc/ffu3uVuze6+yvufkPTe8wsDbgMuAkYY2ZTIljVtQQhVEeLsAlbIp81s3fNrMLMfhPWgpmNMrOXzGyXmZWb2QNmltHyw81siJntM7NBzeadFLaAepnZ6LAlVBV+ziMt1j86fD7TzFaHradtZvbViH+ZkvQUEJJQzKwAmAm8BYwFCoDHD/O2jwF7CFoazxN8+be3jtMJWiMPA4+2sfyHCVoyxwMfB85rejvwQ2AoMD6s7zst3+zuO4CXw/c2+STwsLvXAd8DXgAyw1p+1Ua59wKfcff+wETgpfa2TaQ5BYQkiifNrBJ4DXgF+AHQ9Nd3yWHeOwt4xN0bgAeBK82s12GW/5u7V4TLX2BmuS2W+ZG7V7r7FmA+cAKAu6939xfd/YC7lxG0cM5oYz1zCEIBM+sBXAncH75WB4wAhrp7rbu/1sZn1AHHmtkAd69w96XtbJfI+yggJFF81N0z3H2Eu3/O3fcDu8LX8tp6U9jiOAt4IJz1FNAXuLCN5VOBy5uWd/d/AluAq1osuqPZ831Av/D9uWb2cNjdUw38Gchuo7ynCL7cRwIfAqrcfWH42q0ErZGFZrbKzK5v4zM+RtCi2hx2Sc1oYzmRf6OAkES2FthK8CXZlmsI/h88Y2Y7gA0EAdFWN9MlwADgLjPbEb5nWDvLt/RDwIFJ7j6AoIVgrS3o7rUEXVhXh3Xe3+y1He5+g7sPBT4T1jO6lc9Y5O4XA7nAk+HniUREASEJy4Ox7L8M/KeZXWdmA8wsxcxOM7N7wsWuBe4g6AJqenwMuLD5AeJmZgF/AI5rtvypwAlmdlwEZfUnON5RaWbDgK8dZvk/AZ8CLiJobQBgZpebWX44WUEQOg3N32hmvc3sajMbGB63qG65jEh7FBCS0Nz9ceATwPXAdmAn8H3gKTM7GSgEfhP+Rd70eBpYT9Dnf0j4hX4O8D8tll8C/J3ITpG9g+BU3CrgOWDuYep/HWgElrr7pmYvTQUWmNke4GngFnff2MpHXANsCruzPkt4TEMkEqYbBol0b2b2EvCgu/8+1rVIclFAiHRjZjYVeBEocPeaWNcjyUVdTCLdlJnNAf4P+KLCQWJBLQgREWmVWhAiItKqnrEuoDOys7O9sLAw1mWIiMSVJUuWlLt7zuGWi+uAKCwsZPHixbEuQ0QkrpjZ5kiWUxeTiIi0SgEhIiKtUkCIiEirFBAiItIqBYSIiLQqagFhZn8ws1IzW9lsXpaZvRjeivFFM8sM55uZ/dLM1pvZCjM7MVp1iYhIZKLZgrgPOL/FvG8A89x9DDAvnAa4ABgTPm4E7o5iXSIiEoGoXQfh7q+aWWGL2RcDZ4bP5xDcc/fr4fw/heP3v2lmGWaW5+6Hu1WkiEjC23ugnpKq/WyvrKWkaj8lVbWcPS6XSfkZUV1vV18oN7jpS9/dS5rdx3cYwZ2/mhSH8/4tIMzsRoJWBsOHD49utSIiUVbX0MjO6lpKqmrZXhmEQPBzP9vDeVX76/7tfYP69Um4gGhLa7dcbHUUQXe/B7gHYMqUKRppUES6tdq6Boor9rFl9z627t7PtqYv/8qgJbCzupbGFt9kA1N7MTQjlWEZfZkyIpOhGakMzejL0IxUhgzoy+ABfendM/rnGHV1QOxs6joyszygNJxfDBQ0Wy6f4O5fIiLdWn1DIyVVtWyt2Efx7v1BEFTsY+vufWyt2E9ZzYH3Ld+7ZwpDBwZf9qeOzj70vCkE8gamkt6ne/zt3tVVPE1wW8YfhT+fajb/82b2MDAdqNLxBxHpLqpr69hcvo+Nu/YGX/y79x0KgpLKWuqbNQF6pBh5A/tSkJnGWWNzKMhMY/igNPIz0yjISiWnXx/MWus06X6iFhBm9hDBAelsMysGvk0QDI+a2WxgC3B5uPhfgZkE9wHeB1wXrbpERFqz50A9m8r3srF8L5t37WVj+T427drLpvK97Np78H3LZvfrQ0FWKpMLMrno+NQgBLLSKMhKY8jAvvTqkRiXmEXzLKYr23jpnFaWdeCmaNUiIgLB8YANZUEIbNr1/jAo3/P+rqAhA/oyYlAaHzp2MIXZ6RQOSqcoO52CrFTSenePLqBoS46tFJGkUrH3IOvL9rC+dA/vle459Hxb5X6a30Qzt38fCrPTOXtcDoXZ6RQNSqcwO50Rg9KSJgTao9+AiMSlxkZne9X+IATK9h4Kg/fK9ryvS6hPzxRG5vRj8vBMLj+pgFG5QUugcFB6tzkY3F3ptyMi3Zq7U1ZzgHd21LBuZw3v7Khh7Y4a1pfuYX9dw6HlMtJ6MTqnHx86djCjcvoxOjd4DM1IpUdKfBwU7m4UECLSbdTU1rFuZw1rd+xh7Y7qQ6FQse9fF4pl9+vDuCH9uWJaQRACOf0YlduPQem94+bsoHihgBCRLtfQ6Gws38uq7VVBCOwIWgbbKvcfWia9dw+OGdKf8ycO4ZjB/Rk7pD9jB/dnUL8+Maw8uSggRCSqDtQ3sG7HHlZtr2LV9upDobDvYNA91DPFGJXTjxNHZHLV9OGMDcNgWEYqKeoaiikFhIgcNTW1dazeXh0GQRAG60v3HLqQrF+fnhybN4CPTylgwtABTBg6kNG5/bpk2Ag5cgoIEemQfQfrWbW9muVbK1leXMXbxZVs2rXv0Os5/fswYegAzh6Xy4ShA5kwdADDs9LUKogjCggROay6hkbW7qhheXElK7ZWsby4knU7aw4NMjcsI5VJ+QO5fEoBxw4dwIShA8jt3ze2RUunKSBE5H3cnU279rFsawXLwzBYvb2aA/WNAGSm9WJSfgbnThjC8fkDmZSfQU5/HThORAoIkSRXW9fAiuIqlmyuYMnmCpZuqWB3eKFZWu8eTBw2kGtnjGBSfgYnFGSQn5mq00mThAJCJMnsqKo9FAZLtlSwalvVoYPII7PTOXtcLieNyGTy8AzG5PbXRWZJTAEhksDcnffK9vDmht0s3LibJZsrDl1r0KdnCscXZHDDB0Zy0vBMThyRSVZ67xhXLN2JAkIkgTQ2Omt31rBgwy4WbAxCoWlcotz+fZhalMXs04o4aUQm4/MG6PRSaZcCQiSONTQ6a0qqeTMMhEWbdlMZDksxLCOVM47JYfrILKYXDWLEoDQdO5AjooAQiSPuzobyvby+vpx/vFvOmxt2UVNbD8CIQWmce+xgphUNYnpRFgVZaTGuVuKdAkKkmyurOcAb75Xz2rvlvL6+nO1VtQDkZ6Zy4XF5zBg1iGlFWeQNTI1xpZJoFBAi3cy+g/Us3Lj7UCvhnR01AAxM7cUpowZx09nZnDY6mxGD0mNcqSQ6BYRIN7ChbA/z15bx8tpSFmzYzcGGRnr3SGFKYSZfO28sp4/JZsLQgTrlVLqUAkIkBmrrGliwcTfz3ynl5bWlh8YwGp3bj2tnjOADx+QwtTCL1N49YlypJDMFhEgXKa7Yx8try5j/TilvvLeL/XUN9O2Vwimjspl9WhFnjs3VgWXpVhQQIlHi7qzaXs0Lq3fywqodh44lFGSl8vEp+Zw5LpcZIwfRt5daCdI9KSBEjqL6hkYWbtrNC6t28uLqnWyr3E+KwZTCLG6/cDxnjctlZHa6rkeQuKCAEOmkfQfreXVdOS+s3sFL75RSua+OPj1TOH1MDrd8cAznjMvVbTIlLikgRDpg/8EG5q8t5dkV23npnVJq6xoZmNqLc8blcu6EwXzgmBzSeuu/l8Q3/QsWiVBtXQOvrCvj2RUlzFuzk30HG8ju15vLTspn5sQ8phZl0auHxjaSxKGAEGnHgfoG/rGunOfeLuHF1TvZc6CezLReXHzCMD4yKY9pRVn0VChIglJAiLTQ2Ogs3LSbvyzdxl9XllBTW8/A1F5ceFweF04KhrZQS0GSgQJCJLS+tIa5S7fx1LLtbKvcT3rvHpw3cQgfOX4op47K1tDYknQUEJLUyvcc4Oll2/nLW9t4e1sVKQanj8nh1vPHcu6xQ3QlsyQ1BYQknbqGRuatKeXRxVt5ZV0ZDY3OhKEDuP3C8Vx0wlBy+/eNdYki3YICQpLGhrI9PLJ4K08s2Ub5ngMMHtCHG04fyaUnDuOYwf1jXZ5It6OAkIS2/2ADf1tZwsOLtrJw4256pBhnj8vliqkFnHFMjs5AEmmHAkIS0pqSah5csIUnl22jpraewkFpfP38cXzsxGHkDlAXkkgkFBCSMOoaGvn7yh3c/8/NLNy0mz49U5h5XB6fmFrA9KIsjX8kcoQUEBL3dlbX8uCCLTy0cAulNQcYnpXGN2eO5/Ip+WSk9Y51eSJxKyYBYWZfAj4NOPA2cB2QBzwMZAFLgWvc/WAs6pPuz91ZtKmCOf/cxPMrd1Df6Jw5NocfzyjkjGNySNGd10Q6rcsDwsyGATcDx7r7fjN7FLgCmAn83N0fNrPfArOBu7u6Pune6hoa+evbJfzvPzawcls1A/r25FOnFPLJk0dQmK17NIscTbHqYuoJpJpZHZAGlABnA1eFr88BvoMCQkI1tXU8smgrf3x9E9sq9zMyJ50fXHIcl0wepovZRKKkywPC3beZ2Z3AFmA/8AKwBKh09/pwsWJgWGvvN7MbgRsBhg8fHv2CJaZKqvZz3+ubeHDBFmoO1DO9KIvvXjyBs8bmqhtJJMpi0cWUCVwMFAGVwGPABa0s6q29393vAe4BmDJlSqvLSPxbX1rDXfPf4+nl23Fg5nF53HB6EZPyM2JdmkjSaDMgzKyGNr6kAdx9QAfX+UFgo7uXheuZC5wCZJhZz7AVkQ9s7+DnSxxbtb2K38xfz99W7iC1Vw+umTGC608toiArLdaliSSdNgPC3fsDmNl3gR3A/YABVwOdGZdgC3CymaURdDGdAywG5gOXEZzJNAt4qhPrkDizbGslv37pXf5vTSn9+/Tk82eN5rpTi8hK12mqIrESSRfTee4+vdn03Wa2APhJR1bo7gvM7HGCU1nrgbcIuoyeAx42s++H8+7tyOdLfFm4cTe/euld/vFuORlpvfjKh47h2lMKGZjaK9aliSS9SAKiwcyuJvjL3oErgYbOrNTdvw18u8XsDcC0znyuxI+lWyq48/m1vPHeLrL79ea2C8bxyZNHkN5H126KdBeR/G+8CvhF+HDgdf51OqrIEVm9vZqfvrCWee+UMii9N7dfOJ5PnjyCvr10qqpId3PYgHD3TQRnHYl02Htle/jZi+t4bkUJA/r25GvnjeVTpxSqxSDSjR32f6eZHUNwwdpgd59oZpOAi9z9+1GvTuLezupafvbCOh5bspW+vXrw+bNGc8MHRuoYg0gciOTPt/8Fvgb8DsDdV5jZg4ACQtq072A9//vqRn77ynvUNzYy65RCbjprNNn9+sS6NBGJUCQBkebuC1sMlVzf1sKS3BobnSeWFnPnC2vZWX2ACyYO4RsXjGPEII2TJBJvIgmIcjMbRXjRnJldRjB2ksj7vLG+nO8/t4bVJdUcX5DBr686kamFWbEuS0Q6KJKAuIngOoVxZrYN2Ah8MqpVSVzZunsf3312NS+u3smwjFR+ccUJfGTSUI2VJBLnIjmLaQPwQTNLB1LcvSb6ZUk8qK1r4J5XN/Cb+evpkWJ87byxzD6tSKesiiSISM5i6gN8DCgEejYdi3D370a1MunWXnpnJ3c8s5rNu/Zx4aQ8br9wPHkDU2NdlogcRZF0MT0FVBEMyX0guuVId7d19z7ueGY1/7dmJ6Ny0nng09M5dXR2rMsSkSiIJCDy3f38qFci3Vp9QyN/fH0TP31xLSlm3HbBOK47tYjePVNiXZqIREkkAfGGmR3n7m9HvRrpllZvr+Ybc1eworiKD47P5XsfnajuJJEkEElAnAZ8ysw2EnQxGeDuPimqlUnM1dY18Mt57/K7VzeQmdaL31x1IjOPG0KLa2JEJEFFEhCt3e1NEtybG3Zx29y32Vi+l49Pyef/zRxPRpruzSCSTNq7o9wAd68GdFprEqmta+Anf1/LH17fyPCsNB2EFkli7bUgHgQ+THD2khN0LTVxYGQU65IYWFFcyZceWcZ7ZXuZNWMEX79gHGm9NdqqSLJq75ajHw5/FnVdORILdQ2N/Pql9fx6/npy+vXh/tnTOH1MTqzLEpEYi+jPQzPLBMYAfZvmufur0SpKus57ZXv40iPLWFFcxSWTh/GdiyZoKG4RASK7kvrTwC1APrAMOBn4J3B2dEuTaJu7tJjbn1xJ754p3HX1icw8Li/WJYlINxJJC+IWYCrwprufZWbjgDuiW5ZE094D9XzrqVU8sbSYaUVZ/OKKE3Rdg4j8m0gCotbda80MM+vj7u+Y2dioVyZRsaakms8/uJQN5Xu5+Zwx3Hz2aHr20NXQIvLvIgmIYjPLAJ4EXjSzCmB7dMuSaHhs8Va++eRKBqb24oHZ0zlFp6+KSDsiGe77kvDpd8xsPjAQ+HtUq5Kj6mB9I997djX3v7mZU0YN4pdXTtatP0XksNq7UK61W4E1jcfUD9gdlYrkqCqtqeVzf17K4s0V3PiBkdx63lh1KYlIRNprQbR2gVwTXSgXB5ZsruA//ryEmtp6fnnlZC46fmisSxKRONLehXK6QC6OPbpoK9988m3yBqYy5/ppjM8bEOuSRCTORHqh3KUEo7o68A93fzKqVUmHNTY6//3CWu5++T1OH5PNr66crEH2RKRDIrlQ7i5gNPBQOOuzZvYhd78pqpXJEauta+Arjy7nubdLuGr6cL570QQdbxCRDoukBXEGMNHdHcDM5vCvg9XSTZTVHOCGPy1meXEl35w5nk+fXqT7NohIp0QSEGuB4cDmcLoAWBG1iuSIbSjbw7V/WEj5ngP89pMncd6EIbEuSUQSQCQBMQhYY2YLw+mpwJtm9jSAu18UreLk8FZuq2LWH4Jd88iNMzi+ICPGFYlIoogkIL4V9SqkQ97csItPz1nMwNRe3D97GiNz+sW6JBFJIJEERJm7r24+w8zOdPeXo1OSROLF1Tu56cGlDM9K4/7Z0zTYnogcdZGc4vKomd1qgVQz+xXww2gXJm2bu7SYz/55CePzBvDYZ2YoHEQkKiIJiOkEB6nfABYRDNR3ajSLkrY9vqSYrzy2nOlFWTz46elkpusaBxGJjkgCog7YD6QS3FFuo7s3dmalZpZhZo+b2TtmtsbMZphZlpm9aGbvhj8zO7OORPT4kmK+9vhyTh2Vzb2zppLeR/eLFpHoiSQgFhEExFSCq6mvNLPHO7neXwB/d/dxwPHAGuAbwDx3HwPMC6cl1Dwc/vfaKaT27hHrkkQkwUXyJ+hsd18cPt8BXGxm13R0hWY2APgA8CkAdz8IHDSzi4Ezw8XmAC8DX+/oehKJwkFEYqHNFoSZnQ3g7ovNrOXAfXs7sc6RQBnwRzN7y8x+b2bpwGB3LwnXWQLktlHXjWa22MwWl5WVdaKM+PD08u0KBxGJifa6mO5s9vyJFq/d3ol19gROBO5298kEYRNxd5K73+PuU9x9Sk5OTifK6P7mv1PKlx9ZxtTCLIWDiHS59gLC2nje2vSRKAaK3X1BOP04QWDsNLM8gPBnaSfWEfcWbdrNfzywhLFD+vP7WQoHEel67QWEt/G8temIufsOYKuZjQ1nnQOsBp4GZoXzZgFPdXQd8W719mquv28RQ8N7OQzo2yvWJYlIEmrvIPXIcLwla/accLqzNxP6AvCAmfUGNgDXEYTVo2Y2G9gCXN7JdcSlbZX7mfXHhfTr05M/zZ6me0eLSMy0FxAXN3t+Z4vXWk4fEXdfBkxp5aVzOvO58a6mto7Z9y2i9mADT3zuFPIz02JdkogksfZuOfpKVxaS7OobGvn8g2+xvnQP9103jWMG9491SSKS5HQpbjfg7nznmVW8sq6MH116HKeNyY51SSIiEV1JLVF272sb+fObW/jMGSO5YtrwWJcjIgIcQUCEF7PJUfb6+nJ+8Nc1nD9hCF8/b1ysyxEROeSwAWFmp5jZaoLxkjCz483srqhXlgS2Ve7nCw+9xaicfvz048eTkqJ7SItI9xFJC+LnwHnALgB3X04wlpJ0Qm1dA5/78xIO1jfy22tO0sisItLtRNTF5O5bW8xqiEItSeWOZ1axvLiKOy8/nlG6VaiIdEOR/Nm61cxOATy8sO1mwu4m6ZjHFm/loYVb+Y8zR3H+xCGxLkdEpFWRtCA+C9wEDCMYR+mEcFo6YEPZHr799CpOHpnFV88de/g3iIjEyGFbEO5eDlzdBbUkvIP1jdzy8DJ690zh5584gR46KC0i3VgkZzHNMbOMZtOZZvaH6JaVmH764lre3lbFjy6dRN7A1FiXIyLSrki6mCa5e2XThLtXAJOjV1Jieu3dcn73ygaunDZcxx1EJC5EEhApZpbZNGFmWWiIjiNSXVvHVx9bzqicdP7zw+NjXY6ISEQi+aL/KfCGmT0eTl8O/Ff0Sko8P3huDaU1tfzlmlNJ661sFZH4EMlB6j+Z2RLgLIJ7QVzq7qujXlmCeO3dch5etJXPnDGS4wsyDv8GEZFuItI/Z98BKpqWN7Ph7r4lalUliL0H6vnG3BUUZafzpQ8eE+tyRESOyGEDwsy+AHwb2ElwBbUR3HJ0UnRLi3///fxatlXu59HPzKBvL91TWkTiSyQtiFuAse6+K9rFJJKlWyqY889NzJpRyNTCrFiXIyJyxCI5i2krUBXtQhJJfUMjt/9lJYP79+Wr5+lqaRGJT5G0IDYAL5vZc8CBppnu/rOoVRXn7n9zM6tLqrnr6hPpp1FaRSRORfLttSV89A4f0o7S6lp+9sI6Th+TzQW6IE5E4lgkp7ne0RWFJIof/HUNB+ob+e7FEzHTWEsiEr8iOYspB7gVmAD0bZrv7mdHsa649M/3dvHksu3cfPZoirJ1h1YRiW+RHKR+gOA6iCLgDmATsCiKNcWlhkbnu8+uZlhGKp87a3SsyxER6bRIAmKQu98L1Ln7K+5+PXBylOuKO3OXFrOmpJqvXzBO1zyISEKI5CB1XfizxMwuBLYD+dErKf7sP9jAnS+s5fiCDD4yKS/W5YiIHBWRBMT3zWwg8BXgV8AA4EtRrSrO/P4fG9hZfYBfX3WiDkyLSMKI5CymZ8OnVQQD9kkzZTUHuPuV9zh/whBdMS0iCaXNgDCzW939J2b2K4Kxl97H3W+OamVx4q6X13OgvpFbz9cV0yKSWNprQawJfy7uikLiUUnVfh5YsIWPnTiMkTn9Yl2OiMhR1WZAuPszZtYDmOjuX+vCmuLGb+avp7HR+cLZY2JdiojIUdfuaa7u3gCc1EW1xJXiin08smgrH59aQEFWWqzLERE56iI5i+ktM3saeAzY2zTT3edGrao48Jv56zGMz+uiOBFJUJEERBawC2g+tIYDSRsQpdW1PLFkG5dPyWdoRmqsyxERiYpITnO9risKiSd/fGMT9Y2N3HD6yFiXIiISNZEM1tcXmM2/D9Z3fRTr6rZqauv485ubuWBiHoUakE9EElgkYzHdDwwBzgNeIRhmo6azKzazHmb2lpk9G04XmdkCM3vXzB4xs25574mHF26lpraeGz+g1oOIJLZIAmK0u/8nsNfd5wAXAscdhXXfwr+utQD4MfBzdx8DVBC0WrqVg/WN3PvaRmaMHMTxBRmxLkdEJKoiCYimwfoqzWwiMBAo7MxKzSyfIGh+H04bwUHwx8NF5gAf7cw6ouH5VTvYUV3LDR8oinUpIiJRF0lA3GNmmcDtwNPAaoK/9jvjfwhuQtQYTg8CKt29PpwuBoa19kYzu9HMFpvZ4rKysk6WcWTuf3MzBVmpnHlMbpeuV0QkFtoMCDMbDODuv3f3Cnd/1d1Hunuuu/+uoys0sw8Dpe6+pPnsVhb9t/Gfwnrucfcp7j4lJyeno2UcsbU7ali4cTefnD6ClBSN2Coiia+9s5iWm9nbwEPAE+5edZTWeSpwkZnNJDgragBBiyLDzHqGrYh8gvtOdBv3v7mJ3j1T+PiUgliXIiLSJdrrYhoG3AmcDqwzsyfN7BNm1qkrw9z9NnfPd/dC4ArgJXe/GpgPXBYuNgt4qjPrOZpqauv4y9JtfGTSUDLTu+XJVSIiR12bAeHuDe7+fHihXAHwR4IDxxvN7IEo1PJ14Mtmtp7gmMS9UVhHhzy3ooS9Bxu4+uThsS5FRKTLRDLUBu5+0MxWE5yWehJw7NFYubu/DLwcPt8ATDsan3u0zV26jVE56UzWqa0ikkTaPYvJzIab2dfMbCnwLNADuNjdJ3dJdd3All37WLhpN5eemK/biYpIUmnvjnJvEByHeAy40d2T8sZBc98qxgwumdzqWbciIgmrvS6m24BX3b3V002Tgbszd+k2Thk1SKO2ikjSae8g9SvJHA4ASzZXsGX3Pi6dnB/rUkREulwkV1InrWeWb6dPzxTOnzgk1qWIiHQ5BUQbGhudv63cwVljc0nvE9HJXiIiCSXigDCzk83sJTN73cy63UB6R9vizRWU1hxg5qS8WJciIhIT7Z3FNMTddzSb9WXgIoJxk94AnoxybTH117dL6NMzhbPHaWA+EUlO7fWd/NbMlgD/7e61QCVwFcEIrNVdUVy7emOwAAAMGklEQVSsBN1LJZxxTA791L0kIkmqvbOYPgosA541s2uALxKEQxrd8F4NR9NbWyvZWX2Amcepe0lEkle7xyDc/RmCW41mAHOBte7+S3fv2hsxdLH575TSI8U4a6y6l0QkebV3P4iLzOw14CVgJcHIq5eY2UNmNqqrCoyF+WtLOXF4BgPTesW6FBGRmGmvBfF9gtbDx4Afu3ulu38Z+BbwX11RXCyUVteyans1Z6r1ICJJrr0jsFUErYZUoLRppru/G85PSC+vC3rP1L0kIsmuvRbEJQQHpOsJzl5KCi+vLWXwgD6Mz+sf61JERGKqzRaEu5cDv+rCWmKurqGRf6wrZ+ZxeRraW0SSnobaaGbV9mpqDtRz2pjsWJciIhJzCohmFm3cDcD0oqwYVyIiEnsKiGYWbNxN4aA0cgf0jXUpIiIxp4AINTY6izfvZppaDyIigALikPVle6jcV8e0okGxLkVEpFtQQIQWhMcfphWqBSEiAgqIQ5ZuriC3fx8KsnTvaRERUEAcsry4kuMLMnT9g4hISAEB1NTWsaFsL5OGDYx1KSIi3YYCAnh7WxUAx+UrIEREmigggLeLg4CYlJ8R40pERLoPBQSworiK/MxUstJ7x7oUEZFuQwEBrNpexcSh6l4SEWku6QNi/8EGNu/ex9ghGt5bRKS5pA+I9aV7cEcBISLSQtIHxLqdNQAcM1gBISLSnAJiZw29e6RQOCgt1qWIiHQrSR8Qa3fWMCq3Hz17JP2vQkTkfZL+W3HdjhrGDu4X6zJERLqdpA6IPQfq2V5VyxgdfxAR+TddHhBmVmBm881sjZmtMrNbwvlZZvaimb0b/syMdi2byvcCUJSdHu1ViYjEnVi0IOqBr7j7eOBk4CYzOxb4BjDP3ccA88LpqNq0KwiIwkEKCBGRlro8INy9xN2Xhs9rgDXAMOBiYE642Bzgo9GupakFUZitM5hERFqK6TEIMysEJgMLgMHuXgJBiAC5bbznRjNbbGaLy8rKOrX+jeX7GDygD2m9e3bqc0REElHMAsLM+gFPAF909+pI3+fu97j7FHefkpOT06kaNu3aq+4lEZE2xCQgzKwXQTg84O5zw9k7zSwvfD0PKI12HZvK9+oAtYhIG2JxFpMB9wJr3P1nzV56GpgVPp8FPBXNOqpr69i19yCFCggRkVbFovP9VOAa4G0zWxbO+3/Aj4BHzWw2sAW4PJpFbC7fB6AhNkRE2tDlAeHurwHWxsvndFUd2yr3A5CfqYAQEWlN0l5JXVIVBETewL4xrkREpHtK2oDYUVVL754pus2oiEgbkjYgSqpqyRvYl+CYuYiItJS0AbGjqpYhA9S9JCLSlqQNiJ01tQxWQIiItClpA6Ji70EdfxARaUdSBkRDo1NdW8/A1F6xLkVEpNtKyoCo3l8HQEaaAkJEpC1JGRCVYUBkpqmLSUSkLUkZEBX7DgIwUC0IEZE2JWVAVO0Lu5h0DEJEpE1JGRCV+4MWRIa6mERE2pScAaEWhIjIYSVlQAzLSOXcYwczQAEhItKmpLwZ87kThnDuhCGxLkNEpFtLyhaEiIgcngJCRERapYAQEZFWKSBERKRVCggREWmVAkJERFqlgBARkVYpIEREpFXm7rGuocPMrAzY3MG3ZwPlR7GceKBtTg7a5uTQmW0e4e45h1sorgOiM8xssbtPiXUdXUnbnBy0zcmhK7ZZXUwiItIqBYSIiLQqmQPinlgXEAPa5uSgbU4OUd/mpD0GISIi7UvmFoSIiLRDASEiIq1KyoAws/PNbK2ZrTezb8S6nqPFzArMbL6ZrTGzVWZ2Szg/y8xeNLN3w5+Z4Xwzs1+Gv4cVZnZibLegY8ysh5m9ZWbPhtNFZrYg3N5HzKx3OL9POL0+fL0wlnV3lJllmNnjZvZOuK9nJME+/lL4b3qlmT1kZn0TcT+b2R/MrNTMVjabd8T71sxmhcu/a2azOlpP0gWEmfUAfgNcABwLXGlmx8a2qqOmHviKu48HTgZuCrftG8A8dx8DzAunIfgdjAkfNwJ3d33JR8UtwJpm0z8Gfh5ubwUwO5w/G6hw99HAz8Pl4tEvgL+7+zjgeIJtT9h9bGbDgJuBKe4+EegBXEFi7uf7gPNbzDuifWtmWcC3genANODbTaFyxNw9qR7ADOD5ZtO3AbfFuq4obetTwIeAtUBeOC8PWBs+/x1wZbPlDy0XLw8gP/xPczbwLGAEV5f2bLm/geeBGeHznuFyFuttOMLtHQBsbFl3gu/jYcBWICvcb88C5yXqfgYKgZUd3bfAlcDvms1/33JH8ki6FgT/+sfWpDicl1DCZvVkYAEw2N1LAMKfueFiifC7+B/gVqAxnB4EVLp7fTjdfJsObW/4elW4fDwZCZQBfwy71X5vZukk8D52923AncAWoIRgvy0hsfdzc0e6b4/aPk/GgLBW5iXUub5m1g94Aviiu1e3t2gr8+Lmd2FmHwZK3X1J89mtLOoRvBYvegInAne7+2RgL//qcmhN3G9z2D1yMVAEDAXSCbpXWkqk/RyJtrbzqG1/MgZEMVDQbDof2B6jWo46M+tFEA4PuPvccPZOM8sLX88DSsP58f67OBW4yMw2AQ8TdDP9D5BhZj3DZZpv06HtDV8fCOzuyoKPgmKg2N0XhNOPEwRGou5jgA8CG929zN3rgLnAKST2fm7uSPftUdvnyRgQi4Ax4RkQvQkOdj0d45qOCjMz4F5gjbv/rNlLTwNNZzLMIjg20TT/2vBsiJOBqqambDxw99vcPd/dCwn240vufjUwH7gsXKzl9jb9Hi4Ll4+rvyzdfQew1czGhrPOAVaToPs4tAU42czSwn/jTducsPu5hSPdt88D55pZZtj6Ojecd+RifUAmRgeBZgLrgPeAb8a6nqO4XacRNCVXAMvCx0yC/td5wLvhz6xweSM4o+s94G2Cs0Rivh0d3PYzgWfD5yOBhcB64DGgTzi/bzi9Pnx9ZKzr7uC2ngAsDvfzk0Bmou9j4A7gHWAlcD/QJxH3M/AQwXGWOoKWwOyO7Fvg+nD71wPXdbQeDbUhIiKtSsYuJhERiYACQkREWqWAEBGRVikgRESkVQoIERFplQJCEpaZ/dDMzjSzj9oRjtprZjnhSKBvmdnpLV572YLRgJeFj8va+pzDrOOLZpbWkfeKdAUFhCSy6QRjUZ0B/OMI33sO8I67T3b31t57tbufED4e72B9XwSOKCCaXTksEnUKCEk4ZvbfZrYCmAr8E/g0cLeZfauVZUeY2bxwPP15ZjbczE4AfgLMDFsIqRGu95NmtjB8z+/CoeUxs7vNbHF4P4M7wnk3E4wrNN/M5ofz9jT7rMvM7L7w+X1m9rNwuR+bWXp434BFYQvn4nC5Cc3Wv8LMxnT0dygCuie1JCgzmwZcA3wZeNndT21juWeAx919jpldD1zk7h81s08RXJn6+Vbe8zLBsMr7w1nnEIyw+RPgUnevM7O7gDfd/U9mluXuu8PAmAfc7O4rwjGkprh7efi5e9y9X/j8MuDD7v6pMCiygYvdvcHMfgCsdvc/m1kGwdXCk4Efhet8IBxGpoe7N9UocsTUXJVENZlgqJFxBOP2tGUGcGn4/H6CL/lIXO3ui5smzOxK4CRgUTBcEKn8a1C1j5vZjQT/3/IIblS1IsL1NHnM3RvC5+cSDFL41XC6LzCcoLX0TTPLB+a6+7tHuA6R91FASEIJu4fuIxjBspygj9/MbBnBTWQO9xd1R5vUBsxx99ta1FMEfBWY6u4VYWugbwTrbrnM3hbr+pi7r22xzBozWwBcCDxvZp9295eOcDtEDtExCEko7r7M3U8gGIzxWOAl4LzwYHJr4fAGwUiwAFcDr3Vw1fOAy8wsFw7dR3gEwR3g9gJVZjaY99/HoAbo32x6p5mNN7MU4JJ21vU88IVwZFPMbHL4cySwwd1/STDS56QObosIoICQBGRmOQT3JG4Exrl7e11MNwPXhQe1ryG4v/URC9dxO/BC+FkvEtwmcjnwFrAK+APwerO33QP8rekgNcGNf54lCLX2huT+HtALWGHBze2/F87/BLAybC2NA/7UkW0RaaKD1CIi0iq1IEREpFUKCBERaZUCQkREWqWAEBGRVikgRESkVQoIERFplQJCRERa9f8BCTEBf6EYqYEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_variance_vs_features(singular_values, cutoff):\n",
    "    evr = np.array([singular_values[i]**2 / sum(singular_values**2) for i in range(cutoff)])\n",
    "    var = np.cumsum(evr*100)\n",
    "    plt.ylabel('% Variance Explained')\n",
    "    plt.xlabel('# of Features')\n",
    "    plt.title('PCA Analysis')\n",
    "    plt.style.context('seaborn-whitegrid')\n",
    "    plt.plot(var)\n",
    "    \n",
    "plot_variance_vs_features(model.projection.s, max_cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good news is this curve is very steep in the beginning, which shows that a lot of information is conveyed in the first components. However, there is no plateau that we can use to choose a cutoff.\n",
    "\n",
    "**So, instead, we will break off a validaton set and use classifier performance to tune this hyperparameter.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X, review_times, y):\n",
    "    X = np.hstack((X, review_times))\n",
    "    X_train, X_test, y_train, y_test = ms.train_test_split(X, y, test_size=0.2, random_state = 195)\n",
    "    rfor = RandomForestClassifier(n_estimators=51, random_state=195)\n",
    "    rfor.fit(X_train, y_train)\n",
    "    return rfor.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3: Choosing the number of components via the downstream task\n",
    "\n",
    "In some ways, this method of choosing the number of components is even better than the plateau method, because we are optimizing directly on the machine learning task rather than something intrinsic to the dataset. In the cell below, we will call `evaluate model` on models between 10 and 30 PCA components. For each model, we will train an `LsiModel`, compute the $V$ matrix (right singular vectors), call `densify` on that, and pass the dense matrix to evaluate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(10,31):\n",
    "    model = LsiModel(reviews_bow, i, reviews_dict)\n",
    "    densified_v = densify(model[reviews_bow], i)\n",
    "    results.append(evaluate_model(densified_v, reviews_times, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6691,\n",
       " 0.6662,\n",
       " 0.66485,\n",
       " 0.6719,\n",
       " 0.676,\n",
       " 0.6805,\n",
       " 0.68345,\n",
       " 0.69185,\n",
       " 0.6941,\n",
       " 0.68765,\n",
       " 0.6906,\n",
       " 0.6858,\n",
       " 0.6884,\n",
       " 0.6874,\n",
       " 0.6915,\n",
       " 0.69165,\n",
       " 0.6946,\n",
       " 0.6894,\n",
       " 0.69045,\n",
       " 0.6908,\n",
       " 0.6955]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77575\n"
     ]
    }
   ],
   "source": [
    "# final_sppmi_test\n",
    "final_sppmi = evaluate_model(vecs_sppmi, reviews_times, y)\n",
    "print(final_sppmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7404\n"
     ]
    }
   ],
   "source": [
    "# final_sgns_test (5 points)\n",
    "final_sgns = evaluate_model(vecs_sgns, reviews_times, y)\n",
    "print(final_sgns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This is one of the assignments of CIS 545 Big Data Analytics at University of Pennsylvania during Spring 2019."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
